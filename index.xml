<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Roman Ring on Roman Ring</title>
    <link>http://inoryy.com/</link>
    <description>Recent content in Roman Ring on Roman Ring</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018--2019</copyright>
    <lastBuildDate>Sun, 06 Jan 2019 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>DeepMind, StarCraft II, and The Next Big Thing in AI</title>
      <link>http://inoryy.com/post/deepmind-starcraft2-next-big-thing-ai/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0200</pubDate>
      
      <guid>http://inoryy.com/post/deepmind-starcraft2-next-big-thing-ai/</guid>
      <description>

&lt;p&gt;Couple of days ago out of the blue DeepMind announced a StarCraft II related event, with many of the employees being quite excited about it on twitter. In just a few hours we will see what DeepMind has in store for us, but in the meantime let&amp;rsquo;s take a step back and review how we got here and why it is so important.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: the (amazing) event has finished and DeepMind have released a &lt;a href=&#34;https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/&#34; target=&#34;_blank&#34;&gt;very detailed write-up&lt;/a&gt;. See below for my thoughts on the event and the write-up.&lt;/p&gt;

&lt;h1 id=&#34;deepmind&#34;&gt;DeepMind&lt;/h1&gt;

&lt;p&gt;At the end of 2013, DeepMind was a small and relatively unknown startup, founded three years prior with one simple goal: &amp;ldquo;to solve intelligence&amp;rdquo;. Although not yet famous with the general public, they have made a lot of noise in the world of deep reinforcement learning by introducing &lt;a href=&#34;https://deepmind.com/research/dqn/&#34; target=&#34;_blank&#34;&gt;DQN&lt;/a&gt; - an algorithm, capable of matching or even surpassing humans at playing the Atari video games, while observing the game from raw pixels similar to how a human would. In the early 2014 DeepMind was bought by Google Inc. for $500 million, many speculating that deal was ensured by this one achievement.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kuz/DeepMind-Atari-Deep-Q-Learner/master/gifs/breakout.gif&#34;  /&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;
      &lt;small&gt;Atari game of Breakout, played by DQN agent. &lt;a href=&#34;https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner&#34;&gt;Source&lt;/a&gt;.&lt;/small&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In March of 2016 DeepMind did it again, with their &lt;a href=&#34;https://deepmind.com/research/alphago/&#34; target=&#34;_blank&#34;&gt;AlphaGo&lt;/a&gt; program decisively winning against Go world champion Lee Sedol, who held the title for a great number of years. While in hindsight this seems as a sure thing, three years ago this win came almost out of nowhere, with many researchers at the time were predicting that AI will not match human experts for another 10 years at least.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;https://i.imgur.com/NNIEdt5.jpg&#34;  /&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;
      &lt;small&gt;DeepMind&#39;s AlphaGo winning vs Lee Sedol. &lt;a href=&#34;https://www.engadget.com/2016/03/12/watch-alphago-vs-lee-sedol-round-3-live-right-now/&#34;&gt;Source&lt;/a&gt;.&lt;/small&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;starcraft-ii&#34;&gt;StarCraft II&lt;/h1&gt;

&lt;p&gt;Shortly after DeepMind&amp;rsquo;s AlphaGo victory, DeepMind were already prepared to announce the next challenge they will be tackling: the StarCraft II video game. StarCraft II is a real-time strategy game that requires making split second decisions on strategical, tactical, and economical levels, with short-term and long-term goals, throughout the whole duration of the game. Together with its predecessor StarCraft: Brood War, it has a rich competitive e-sports history spanning over 20 years, and is actively played by millions to this day.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;https://i.imgur.com/yaOixgL.png&#34;  /&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;
      &lt;small&gt;Serral securing his world championship title at WCS 2018. &lt;a href=&#34;https://youtu.be/ZO9kqMGK190&#34;&gt;Source&lt;/a&gt;.&lt;/small&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;With much bigger state and action spaces, real-time component, partial observability, and longer game duration, this is a much more difficult task than any attempted before. In fact many AI researchers have actually attempted to tackle it, notably the StarCraft: Brood War bot &lt;a href=&#34;https://arstechnica.com/gaming/2011/01/skynet-meets-the-swarm-how-the-berkeley-overmind-won-the-2010-starcraft-ai-competition/&#34; target=&#34;_blank&#34;&gt;OverMind&lt;/a&gt; was a collaborative effort of many talented scientists. Yet, when pitted against human players, the AI champion would be easily defeated even by relative novices to the game.&lt;/p&gt;

&lt;p&gt;This massive challenge alone wasn&amp;rsquo;t enough for DeepMind researchers, they decided to go one step further. They wanted to not only challenge human experts, but do so &amp;ldquo;on their terms&amp;rdquo;. The AI will be observing the game in a similar way to humans, through image-like features, and will be acting by emulating keyboard and mouse commands as close as possible.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;https://i.imgur.com/9FQkrkJ.jpg&#34;  /&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;
      &lt;small&gt;On the right: StarCraft II game as seen by the AI. &lt;a href=&#34;https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/&#34;&gt;Source&lt;/a&gt;.&lt;/small&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Furthermore, the AI is limited in many aspects to match human experiences, e.g. it is allowed to take a limited number of actions per minute (APM) and can only access the same information a human would see on the screen or minimap. This means that the AI must learn to use in-game camera in order to effectively play.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;https://storage.googleapis.com/deepmind-live-cms/documents/Oriol-Fig-Anim-170809-Optimised-r03.gif&#34;  /&gt;
  &lt;figcaption style=&#34;text-align: center;&#34;&gt;
      &lt;small&gt;StarCraft II AI emulates actions as close to humans as possible. &lt;a href=&#34;https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/&#34;&gt;Source&lt;/a&gt;.&lt;/small&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;the-next-big-thing-in-ai&#34;&gt;The Next Big Thing in AI&lt;/h1&gt;

&lt;p&gt;A StarCraft II player might wonder what is special about this if there were relatively good AIs built into the game from release. The AI you encounter in-game acts based on some pre-defined set of rules, which means it can only play as well as the person who programmed it, reacting only to foreseen events. This makes the AI inflexible, and easy to exploit, among other things.&lt;/p&gt;

&lt;p&gt;In contrast, the AI DeepMind is working on learns the game by itself, essentially from scratch. At each step all it has is the game state it observed, the actions it can take, and some reward stimulus it received for a previous action. In general the approach is called reinforcement learning and &lt;a href=&#34;http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; I give a brief overview of the field.&lt;/p&gt;

&lt;p&gt;To get a feel for what it&amp;rsquo;s like for an RL agent to &amp;ldquo;learn&amp;rdquo; the game, here is a video of an agent attempting to tackle a set of minigames released by DeepMind. On the left you can see an agent that just started the process and is exploring all the various actions it can take, and on the right is the same agent after a fair bit of such experiments (about 50 million steps on average).&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/gEyBzcPU5-w&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;An outside reader might wonder why AI researchers focus so much on games to begin with. The answer is simple: real-world is just too complex for an AI system at this point - one must first learn to walk before he gets to fly. Specifically, games have easily defined rules that can be described to an AI system and are typically easy to run in parallel on a computer, speeding up the learning process.&lt;/p&gt;

&lt;p&gt;This however does not mean that current work will be useless in the future. For example, while &amp;ldquo;playing&amp;rdquo; with StarCraft II, DeepMind researchers produced a number of general-purpose results such as the &lt;a href=&#34;https://deepmind.com/blog/population-based-training-neural-networks/&#34; target=&#34;_blank&#34;&gt;population based training&lt;/a&gt;, and &lt;a href=&#34;https://openreview.net/forum?id=HkxaFoC9KQ&#34; target=&#34;_blank&#34;&gt;relational deep reinforcement learning&lt;/a&gt;. So while developing an AI capable of winning in StarCraft II with human-like restrictions &lt;em&gt;probably&lt;/em&gt; won&amp;rsquo;t give us &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_general_intelligence&#34; target=&#34;_blank&#34;&gt;AGI&lt;/a&gt;, it will definitely bring us an inch closer.&lt;/p&gt;

&lt;h1 id=&#34;predictions-and-speculations&#34;&gt;Predictions and Speculations&lt;/h1&gt;

&lt;p&gt;In this section I will attempt to predict some of the things we will see at the event, along with the nitty-gritty details &amp;ldquo;under the hood&amp;rdquo; of the AI, which might get a bit technical. Note that I have no insider information, this is simply my guesses.&lt;/p&gt;

&lt;p&gt;For the event itself I believe it will be a Protoss vs Protoss showmatch between DeepMind&amp;rsquo;s AI and some high level player, either ex-pro or possibly even current pro, however not best of the best tier. I would be &lt;em&gt;really&lt;/em&gt; surprised if they are already at a level to challenge world champions.&lt;/p&gt;

&lt;p&gt;The core algorithm they will employ will be an &lt;code&gt;actor-critic&lt;/code&gt; variant called &lt;a href=&#34;https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/&#34; target=&#34;_blank&#34;&gt;IMPALA&lt;/a&gt;, fused with &lt;code&gt;attention&lt;/code&gt; mechanism as described in &lt;a href=&#34;https://openreview.net/forum?id=HkxaFoC9KQ&#34; target=&#34;_blank&#34;&gt;Relational DRL&lt;/a&gt;. Network architecture will have &lt;code&gt;residual + convolutional&lt;/code&gt; state encoder component and a number of &lt;code&gt;Conv LSTM&lt;/code&gt; blocks generating the policy. It will also have a fair bit of &lt;code&gt;imitation learning&lt;/code&gt; based pre-training prior to the DRL loop.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;IMPALA&lt;/code&gt; algorithm is essentially a distributed, asyncronous version of &lt;code&gt;A2C&lt;/code&gt; I&amp;rsquo;ve described in &lt;a href=&#34;http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; blog with a number of fixes (e.g. importance weighting) to address its &amp;ldquo;off-policyness&amp;rdquo; - the fact that samples are gathered with a different policy than the one being currently trained. If you&amp;rsquo;re familiar with &lt;a href=&#34;https://blog.openai.com/openai-five/&#34; target=&#34;_blank&#34;&gt;openAI Five&lt;/a&gt; then the core idea is similar to their asyncronous PPO: a (massively) distributed advantage actor-critic variant with tricks to correct for the distributional drift.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;attention&lt;/code&gt; mechanism has revolutionized the world of NLP, especially after the &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;Attention is All You Need&lt;/a&gt;&amp;rdquo; article came out. In short, if we&amp;rsquo;re dealing with sequential input / output then attention mechanism acts as importance weight for items in the given sequence when generating the next output. In NLP this is most often used for machine translation to give attention to words at different parts of the sentence. In StarCraft II this could be used to ensure the agent can quickly switch between tactical and strategical decision making. DeepMind has recently applied it to DRL in their &lt;a href=&#34;https://openreview.net/forum?id=HkxaFoC9KQ&#34; target=&#34;_blank&#34;&gt;Relational DRL&lt;/a&gt; article.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;residual + convolutional&lt;/code&gt; architecture makes sense to use given our state space, whereas &lt;code&gt;Conv LSTM&lt;/code&gt; cells (NB! different from &lt;em&gt;CNN&lt;/em&gt; LSTM) are starting to gain traction in DRL world, especially when observations are rich with spatial information.&lt;/p&gt;

&lt;p&gt;During Blizzcon 2018 presentation Oriol Vinyals mentioned that they&amp;rsquo;ve used &lt;code&gt;imitation learning&lt;/code&gt; for their agent - and it of course make sense, given that Blizzard is providing free access (to everybody!) to their massive dataset of replays. Of course it would be impossible to succesfully train a full agent from replays alone, but I think it&amp;rsquo;s reasonable to use it for NN weights pre-training.&lt;/p&gt;

&lt;p&gt;I also wouldn&amp;rsquo;t be surprised if instead of fully end-to-end approach DeepMind will rely on something modular, e.g. as described &lt;a href=&#34;https://arxiv.org/abs/1811.03555&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, where individual modules are responsible for specific subsets of the game. Specifically, perhaps DeepMind relies on pre-defined rulesets for scouting and makes use of a simplified game engine for battle simulations that are used in MCTS solver to determine whether they should attack or fall back.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Hopefully you&amp;rsquo;re now as excited as I am about the upcoming event. Whether the AI is fully end to end or not and whether they will be able to win vs human experts or not, it is still a massive endeavor and should provide for a good show. And if you&amp;rsquo;d like to get into developing StarCraft II based AIs yourself, join our &lt;a href=&#34;https://discordapp.com/invite/Emm5Ztz&#34; target=&#34;_blank&#34;&gt;SC2AI community on discord&lt;/a&gt;!&lt;/p&gt;

&lt;h1 id=&#34;post-event-write-up&#34;&gt;Post-Event Write-Up&lt;/h1&gt;

&lt;p&gt;The stream was very exciting to watch, both as a player and a researcher. It was funny to see AlphaStar opt for wild strategies and then come out on top. It was also interesting to see it make mistakes such as killing its own units, very AI and human-like behavior at the same time.&lt;/p&gt;

&lt;p&gt;To me the most impressive was the micro, and not just due to its ability to make split second decisions. The way AlphaStar knew how to pull back damaged stalkers to regenerate shields, the way it pulled its workers when it saw Oracles - really mind boggling that a single end-to-end neural network is capable of such a rich variety of tactical and somewhat long-term decision-making.&lt;/p&gt;

&lt;p&gt;However, AlphaStar is still quite far from conquering StarCraft II universe. First, while Mana is no doubt a great player, he is not quite world champion caliber. Second, this is still a single matchup, whereas any human player would be expected to play vs all three races on the same level. Third, I&amp;rsquo;m not sure how I feel about having players go against a pool of AlphaStar(s) - I think it definitely makes sense to use for training, but during inference I&amp;rsquo;d prefer to see a single version used throughout the matches. Overall I would say AlphaStar right now is closer to the AlphaGo version that played vs Fan Hui than the one that won vs Lee Sedol.&lt;/p&gt;

&lt;p&gt;Seems that I&amp;rsquo;ve correctly predicted the match-up and level of play, along with some of the approaches. Specifically, AlphaStar does indeed rely on &lt;code&gt;imitation learning&lt;/code&gt;, &lt;code&gt;IMPALA&lt;/code&gt;, and &lt;code&gt;attention&lt;/code&gt; mechanism, though not quite as described in Relational DRL article. They also indeed use &lt;code&gt;LSTM&lt;/code&gt;, but I am not so sure with regards to &lt;code&gt;convolutional&lt;/code&gt; layers - there seems to be a bit of confusion as to what interface they ended up using. I&amp;rsquo;ve also briefly mentioned &lt;code&gt;population based training&lt;/code&gt; - seems that DeepMind uses an advanced variant of it, hopefully we will see an article about it soon.&lt;/p&gt;

&lt;p&gt;Of the things I&amp;rsquo;ve missed is the &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;transformer&lt;/a&gt; body, which is a state-of-the-art architecture in machine translation. Very surprised to see it applied in DRL. They also use a relatively novel baseline for the &lt;code&gt;advantage function&lt;/code&gt; in the PG loss, which they pulled from the &lt;a href=&#34;https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf&#34; target=&#34;_blank&#34;&gt;Counterfactual Multi-Agent Policy Gradients&lt;/a&gt; article. This is noteworthy because for the longest time the value estimate of next state for baseline was the go-to approach of pretty much everybody in DRL.&lt;/p&gt;

&lt;p&gt;Finally, they apply &lt;a href=&#34;https://arxiv.org/abs/1506.03134&#34; target=&#34;_blank&#34;&gt;pointer network&lt;/a&gt; to the policy output, most likely as an efficient way to deal with variable length of action arguments. To me the use of &lt;code&gt;pointer networks&lt;/code&gt; was quite surprising and somewhat ironic - I have actually &lt;a href=&#34;http://inoryy.com/files/pointer_networks_essay.pdf&#34; target=&#34;_blank&#34;&gt;written an essay&lt;/a&gt; on this article and while it was an interesting subject, it never crossed my mind it could be applied in such a way to DRL policies. Although in retrospect I guess it makes sense.&lt;/p&gt;

&lt;p&gt;During training they also rely on &lt;a href=&#34;http://proceedings.mlr.press/v80/oh18b/oh18b.pdf&#34; target=&#34;_blank&#34;&gt;self-imitation&lt;/a&gt; and &lt;code&gt;experience replay&lt;/code&gt;, which is quite interesting - seems they have finally perfected the combination of &lt;code&gt;actor-critic methods&lt;/code&gt;, which are traditionally seen as &lt;code&gt;on-policy&lt;/code&gt;, with the benefits of &lt;code&gt;off-policy&lt;/code&gt; algorithms. Finally, they use &lt;a href=&#34;https://arxiv.org/pdf/1511.06295.pdf&#34; target=&#34;_blank&#34;&gt;policy distillation&lt;/a&gt; which is probably how they were able to fit the final agents into a single machine for inference.&lt;/p&gt;

&lt;p&gt;If by now your head is spinning from all the terminology, don&amp;rsquo;t worry - mine is too. The takeaway message is that it took an impressive amount of very advanced approaches to achieve the level of play we have seen today and I am curious to see what happens next.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning with TensorFlow 2.0</title>
      <link>http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0200</pubDate>
      
      <guid>http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/</guid>
      <description>

&lt;p&gt;In this tutorial I will showcase the upcoming TensorFlow 2.0 features through the lense of deep reinforcement learning (DRL) by implementing an advantage actor-critic (A2C) agent to solve the classic CartPole-v0 environment. While the goal is to showcase TensorFlow 2.0, I will do my best to make the DRL aspect approachable as well, including a brief overview of the field.&lt;/p&gt;

&lt;p&gt;In fact since the main focus of the 2.0 release is making developers’ lives easier, it’s a great time to get into DRL with TensorFlow - our full agent source is under 150 lines! Code is available as a notebook &lt;a href=&#34;https://github.com/inoryy/tensorflow2-deep-reinforcement-learning&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and online on Google Colab &lt;a href=&#34;https://colab.research.google.com/drive/12QvW7VZSzoaF-Org-u-N6aiTdBN5ohNA&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;As TensorFlow 2.0 is still in experimental stage, I recommend installing it in a separate (virtual) environment. I prefer &lt;a href=&#34;https://www.anaconda.com/download&#34; target=&#34;_blank&#34;&gt;Anaconda&lt;/a&gt;, so I’ll illustrate with it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; conda create -n tf2 python=3.6
&amp;gt; source activate tf2
&amp;gt; pip install tf-nightly-2.0-preview # tf-nightly-gpu-2.0-preview for GPU version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s quickly verify that everything works as expected:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
&amp;gt;&amp;gt;&amp;gt; print(tf.__version__)
1.13.0-dev20190117
&amp;gt;&amp;gt;&amp;gt; print(tf.executing_eagerly())
True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don’t worry about the 1.13.x version, just means that it’s an early preview. What’s important to note here is that we’re in eager mode by default!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; print(tf.reduce_sum([1, 2, 3, 4, 5]))
tf.Tensor(15, shape=(), dtype=int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’re not yet familiar with eager mode, then in essence it means that computation is executed at runtime, rather than through a pre-compiled graph. You can find a good overview in the &lt;a href=&#34;https://www.tensorflow.org/tutorials/eager/eager_basics&#34; target=&#34;_blank&#34;&gt;TensorFlow documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;deep-reinforcement-learning&#34;&gt;Deep Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Generally speaking, reinforcement learning is a high level framework for solving sequential decision making problems. A RL &lt;code&gt;agent&lt;/code&gt; navigates an &lt;code&gt;environment&lt;/code&gt; by taking &lt;code&gt;actions&lt;/code&gt; based on some &lt;code&gt;observations&lt;/code&gt;, receiving &lt;code&gt;rewards&lt;/code&gt; as a result. Most RL algorithms work by maximizing sum of rewards an agent collects in a &lt;code&gt;trajectory&lt;/code&gt;, e.g. during one in-game round.&lt;/p&gt;

&lt;p&gt;The output of an RL based algorithm is typically a &lt;code&gt;policy&lt;/code&gt; - a function that maps states to actions. A valid policy can be as simple as a hard-coded no-op action. Stochastic policy is represented as a conditional probability distribution of actions, given some state.
&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://i.imgur.com/fUcDHVt.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;actor-critic-methods&#34;&gt;Actor-Critic Methods&lt;/h4&gt;

&lt;p&gt;RL algorithms are often grouped based on the objective function they are optimized with. &lt;code&gt;Value-based&lt;/code&gt; methods, such as &lt;a href=&#34;https://deepmind.com/research/dqn/&#34; target=&#34;_blank&#34;&gt;DQN&lt;/a&gt;, work by reducing the error of the expected state-action values.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Policy Gradients&lt;/code&gt; methods directly optimize the policy itself by adjusting its parameters, typically via gradient descent. Calculating gradients fully is usually intractable, so instead they are often estimated via monte-carlo methods.&lt;/p&gt;

&lt;p&gt;The most popular approach is a hybrid of the two: &lt;code&gt;actor-critic&lt;/code&gt; methods, where agents policy is optimized through policy gradients, while value based method is used as a bootstrap for the expected value estimates.&lt;/p&gt;

&lt;h4 id=&#34;deep-actor-critic-methods&#34;&gt;Deep Actor-Critic Methods&lt;/h4&gt;

&lt;p&gt;While much of the fundamental RL theory was developed on the tabular cases, modern RL is almost exclusively done with function approximators, such as artificial neural networks. Specifically, an RL algorithm is considered “deep” if the policy and value functions are approximated with &lt;code&gt;deep neural networks&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/319121340_Enabling_Cognitive_Smart_Cities_Using_Big_Data_and_Machine_Learning_Approaches_and_Challenges&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://i.imgur.com/gsXfI91.jpg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;asynchronous-advantage-actor-critic&#34;&gt;(Asynchronous) Advantage Actor-Critic&lt;/h4&gt;

&lt;p&gt;Over the years, a number of improvements have been added to address sample efficiency and stability of the learning process.&lt;/p&gt;

&lt;p&gt;First, gradients are weighted with &lt;code&gt;returns&lt;/code&gt;: discounted future rewards, which somewhat alleviates the credit assignment problem, and resolves theoretical issues with infinite timesteps.&lt;/p&gt;

&lt;p&gt;Second, an &lt;code&gt;advantage function&lt;/code&gt; is used instead of raw returns. Advantage is formed as the difference between returns and some baseline (e.g. state-action estimate) and can be thought of as a measure of how good a given action is compared to some average.&lt;/p&gt;

&lt;p&gt;Third, an additional &lt;code&gt;entropy maximization&lt;/code&gt; term is used in objective function to ensure agent sufficiently explores various policies. In essence, entropy measures how random a probability distribution is, maximized with uniform distribution.&lt;/p&gt;

&lt;p&gt;Finally, &lt;code&gt;multiple workers&lt;/code&gt; are used in &lt;code&gt;parallel&lt;/code&gt; to speed up sample gathering while helping decorrelate them during training.&lt;/p&gt;

&lt;p&gt;Incorporating all of these changes with deep neural networks we arrive at the two of the most popular modern algorithms: (asynchronous) advantage actor critic, or &lt;code&gt;A3C/A2C&lt;/code&gt; for short. The difference between the two is more technical than theoretical: as the name suggests, it boils down to how the parallel workers estimate their gradients and propagate them to the model.
&lt;a href=&#34;https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://i.imgur.com/CL0w8rl.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With this I will wrap up our tour of DRL methods as the focus of the blog post is more on the TensorFlow 2.0 features. Don’t worry if you’re still unsure about the subject, things should become clearer with code examples. If you want to learn more then one good resource to get started is &lt;a href=&#34;https://spinningup.openai.com/en/latest&#34; target=&#34;_blank&#34;&gt;Spinning Up in Deep RL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;advantage-actor-critic-with-tensorflow-2-0&#34;&gt;Advantage Actor-Critic with TensorFlow 2.0&lt;/h2&gt;

&lt;p&gt;Now that we’re more or less on the same page, let’s see what it takes to implement the basis of many modern DRL algorithms: an actor-critic agent, described in previous section. For simplicity, we won’t implement parallel workers, though most of the code will have support for it. An interested reader could then use this as an exercise opportunity.&lt;/p&gt;

&lt;p&gt;As a testbed we will use the &lt;a href=&#34;https://gym.openai.com/envs/CartPole-v0/&#34; target=&#34;_blank&#34;&gt;CartPole-v0&lt;/a&gt; environment. Somewhat simplistic, it&amp;rsquo;s still a great option to get started with. I always rely on it as a sanity check when implementing RL algorithms.&lt;/p&gt;

&lt;h4 id=&#34;policy-value-via-keras-model-api&#34;&gt;Policy &amp;amp; Value via Keras Model API&lt;/h4&gt;

&lt;p&gt;First, let&amp;rsquo;s create the policy and value estimate NNs under a single model class:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import tensorflow as tf
import tensorflow.keras.layers as kl

class ProbabilityDistribution(tf.keras.Model):
    def call(self, logits):
        # sample a random categorical action from given logits
        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)

class Model(tf.keras.Model):
    def __init__(self, num_actions):
        super().__init__(&#39;mlp_policy&#39;)
        # no tf.get_variable(), just simple Keras API
        self.hidden1 = kl.Dense(128, activation=&#39;relu&#39;)
        self.hidden2 = kl.Dense(128, activation=&#39;relu&#39;)
        self.value = kl.Dense(1, name=&#39;value&#39;)
        # logits are unnormalized log probabilities
        self.logits = kl.Dense(num_actions, name=&#39;policy_logits&#39;)
        self.dist = ProbabilityDistribution()

    def call(self, inputs):
        # inputs is a numpy array, convert to Tensor
        x = tf.convert_to_tensor(inputs, dtype=tf.float32)
        # separate hidden layers from the same input tensor
        hidden_logs = self.hidden1(x)
        hidden_vals = self.hidden2(x)
        return self.logits(hidden_logs), self.value(hidden_vals)

    def action_value(self, obs):
        # executes call() under the hood
        logits, value = self.predict(obs)
        action = self.dist.predict(logits)
        # a simpler option, will become clear later why we don&#39;t use it
        # action = tf.random.categorical(logits, 1)
        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And let&amp;rsquo;s verify the model works as expected:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym

env = gym.make(&#39;CartPole-v0&#39;)
model = Model(num_actions=env.action_space.n)

obs = env.reset()
# no feed_dict or tf.Session() needed at all
action, value = model.action_value(obs[None, :])
print(action, value) # [1] [-0.00145713]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Things to note here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model layers and execution path are defined separately&lt;/li&gt;
&lt;li&gt;There is no &amp;ldquo;input&amp;rdquo; layer, model will accept raw numpy arrays&lt;/li&gt;
&lt;li&gt;Two computation paths can be defined in one model via functional API&lt;/li&gt;
&lt;li&gt;A model can contain helper methods such as action sampling&lt;/li&gt;
&lt;li&gt;In eager mode everything works from raw numpy arrays&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;random-agent&#34;&gt;Random Agent&lt;/h4&gt;

&lt;p&gt;Now we can move on to the fun stuff - the &lt;code&gt;A2CAgent&lt;/code&gt; class. First, let&amp;rsquo;s add a &lt;code&gt;test&lt;/code&gt; method that runs through a full episode and returns sum of rewards.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class A2CAgent:
    def __init__(self, model):
        self.model = model

    def test(self, env, render=True):
        obs, done, ep_reward = env.reset(), False, 0
        while not done:
            action, _ = self.model.action_value(obs[None, :])
            obs, reward, done, _ = env.step(action)
            ep_reward += reward
            if render:
                env.render()
        return ep_reward
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see how much our model scores with randomly initialized weights:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;agent = A2CAgent(model)
rewards_sum = agent.test(env)
print(&amp;quot;%d out of 200&amp;quot; % rewards_sum) # 18 out of 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not even close to optimal, time to get to the training part!&lt;/p&gt;

&lt;h4 id=&#34;loss-objective-function&#34;&gt;Loss / Objective Function&lt;/h4&gt;

&lt;p&gt;As I&amp;rsquo;ve described in the DRL overview section, an agent improves its policy through gradient descent based on some loss (objective) function. In actor-critic we train on three objectives: improving policy with advantage weighted gradients plus entropy maximization, and minizing value estimate errors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow.keras.losses as kls
import tensorflow.keras.optimizers as ko

class A2CAgent:
    def __init__(self, model):
        # hyperparameters for loss terms
        self.params = {&#39;value&#39;: 0.5, &#39;entropy&#39;: 0.0001}
        self.model = model
        self.model.compile(
            optimizer=ko.RMSprop(lr=0.0007),
            # define separate losses for policy logits and value estimate
            loss=[self._logits_loss, self._value_loss]
        )

    def test(self, env, render=True):
        # unchanged from previous section
        ...

    def _value_loss(self, returns, value):
        # value loss is typically MSE between value estimates and returns
        return self.params[&#39;value&#39;]*kls.mean_squared_error(returns, value)

    def _logits_loss(self, acts_and_advs, logits):
        # a trick to input actions and advantages through same API
        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)
        # polymorphic CE loss function that supports sparse and weighted options
        # from_logits argument ensures transformation into normalized probabilities
        cross_entropy = kls.CategoricalCrossentropy(from_logits=True)
        # policy loss is defined by policy gradients, weighted by advantages
        # note: we only calculate the loss on the actions we&#39;ve actually taken
        # thus under the hood a sparse version of CE loss will be executed
        actions = tf.cast(actions, tf.int32)
        policy_loss = cross_entropy(actions, logits, sample_weight=advantages)
        # entropy loss can be calculated via CE over itself
        entropy_loss = cross_entropy(logits, logits)
        # here signs are flipped because optimizer minimizes
        return policy_loss - self.params[&#39;entropy&#39;]*entropy_loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done with the objective functions! Note how compact the code is: there&amp;rsquo;s almost more comment lines than code itself.&lt;/p&gt;

&lt;h4 id=&#34;agent-training-loop&#34;&gt;Agent Training Loop&lt;/h4&gt;

&lt;p&gt;Finally, there&amp;rsquo;s the train loop itself. It&amp;rsquo;s relatively long, but fairly straightforward: collect samples, calculate returns and advantages, and train the model on them.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class A2CAgent:
    def __init__(self, model):
        # hyperparameters for loss terms
        self.params = {&#39;value&#39;: 0.5, &#39;entropy&#39;: 0.0001, &#39;gamma&#39;: 0.99}
        # unchanged from previous section
        ...
        
   def train(self, env, batch_sz=32, updates=1000):
        # storage helpers for a single batch of data
        actions = np.empty((batch_sz,), dtype=np.int32)
        rewards, dones, values = np.empty((3, batch_sz))
        observations = np.empty((batch_sz,) + env.observation_space.shape)
        # training loop: collect samples, send to optimizer, repeat updates times
        ep_rews = [0.0]
        next_obs = env.reset()
        for update in range(updates):
            for step in range(batch_sz):
                observations[step] = next_obs.copy()
                actions[step], values[step] = self.model.action_value(next_obs[None, :])
                next_obs, rewards[step], dones[step], _ = env.step(actions[step])

                ep_rews[-1] += rewards[step]
                if dones[step]:
                    ep_rews.append(0.0)
                    next_obs = env.reset()

            _, next_value = self.model.action_value(next_obs[None, :])
            returns, advs = self._returns_advantages(rewards, dones, values, next_value)
            # a trick to input actions and advantages through same API
            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)
            # performs a full training step on the collected batch
            # note: no need to mess around with gradients, Keras API handles it
            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])
        return ep_rews

    def _returns_advantages(self, rewards, dones, values, next_value):
        # next_value is the bootstrap value estimate of a future state (the critic)
        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)
        # returns are calculated as discounted sum of future rewards
        for t in reversed(range(rewards.shape[0])):
            returns[t] = rewards[t] + self.params[&#39;gamma&#39;] * returns[t+1] * (1-dones[t])
        returns = returns[:-1]
        # advantages are returns - baseline, value estimates in our case
        advantages = returns - values
        return returns, advantages

    def test(self, env, render=True):
        # unchanged from previous section
        ...

    def _value_loss(self, returns, value):
        # unchanged from previous section
        ...

    def _logits_loss(self, acts_and_advs, logits):
        # unchanged from previous section
        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;training-results&#34;&gt;Training &amp;amp; Results&lt;/h4&gt;

&lt;p&gt;We&amp;rsquo;re now all set to train our single-worker A2C agent on CartPole-v0! Training process shouldn&amp;rsquo;t take longer than a couple of minutes. After training is complete you should see an agent successfully achieve the target 200 out of 200 score.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rewards_history = agent.train(env)
print(&amp;quot;Finished training, testing...&amp;quot;)
print(&amp;quot;%d out of 200&amp;quot; % agent.test(env)) # 200 out of 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://thumbs.gfycat.com/SoupyConsciousGrayling-size_restricted.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the source code I include some additional helpers that print out running episode rewards and losses, along with basic plotter for the rewards_history.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cFwQgPB.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;static-computational-graph&#34;&gt;Static Computational Graph&lt;/h2&gt;

&lt;p&gt;With all of this eager mode excitement you might wonder if static graph execution is even possible anymore. Of course it is! Moreover, it takes just one additional line to enable it!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with tf.Graph().as_default():
    print(tf.executing_eagerly()) # False

    model = Model(num_actions=env.action_space.n)
    agent = A2CAgent(model)

    rewards_history = agent.train(env)
    print(&amp;quot;Finished training, testing...&amp;quot;)
    print(&amp;quot;%d out of 200&amp;quot; % agent.test(env)) # 200 out of 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s one caveat that during static graph execution we can&amp;rsquo;t just have Tensors laying around, which is why we needed that trick with CategoricalDistribution during model definition. In fact, while I was looking for a way to execute in static mode, I discovered one interesting low level detail about models built through the Keras API&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;one-more-thing&#34;&gt;One More Thing…&lt;/h2&gt;

&lt;p&gt;Remember when I said TensorFlow runs in eager mode by default, even proving it with a code snippet? Well, I lied! Kind of.&lt;/p&gt;

&lt;p&gt;If you use Keras API to build and manage your models then it will attempt to compile them as static graphs under the hood. So what you end up getting is the performance of static computational graphs with flexibility of eager execution.&lt;/p&gt;

&lt;p&gt;You can check status of your model via the &lt;code&gt;model.run_eagerly&lt;/code&gt; flag. You can also force eager mode by setting this flag to &lt;code&gt;True&lt;/code&gt;, though most of the times you probably don’t need to - if Keras detects that there&amp;rsquo;s no way around eager mode, it will back off on its own.&lt;/p&gt;

&lt;p&gt;To illustrate that it’s indeed running as a static graph here&amp;rsquo;s a simple benchmark:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create a 100000 samples batch
env = gym.make(&#39;CartPole-v0&#39;)
obs = np.repeat(env.reset()[None, :], 100000, axis=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;eager-benchmark&#34;&gt;Eager Benchmark&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time

model = Model(env.action_space.n)
model.run_eagerly = True

print(&amp;quot;Eager Execution:  &amp;quot;, tf.executing_eagerly())
print(&amp;quot;Eager Keras Model:&amp;quot;, model.run_eagerly)

_ = model(obs)

######## Results #######

Eager Execution:   True
Eager Keras Model: True
CPU times: user 639 ms, sys: 736 ms, total: 1.38 s
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;static-benchmark&#34;&gt;Static Benchmark&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time

with tf.Graph().as_default():
    model = Model(env.action_space.n)

    print(&amp;quot;Eager Execution:  &amp;quot;, tf.executing_eagerly())
    print(&amp;quot;Eager Keras Model:&amp;quot;, model.run_eagerly)

    _ = model.predict(obs)

######## Results #######

Eager Execution:   False
Eager Keras Model: False
CPU times: user 793 ms, sys: 79.7 ms, total: 873 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;default-benchmark&#34;&gt;Default Benchmark&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time

model = Model(env.action_space.n)

print(&amp;quot;Eager Execution:  &amp;quot;, tf.executing_eagerly())
print(&amp;quot;Eager Keras Model:&amp;quot;, model.run_eagerly)

_ = model.predict(obs)

######## Results #######

Eager Execution:   True
Eager Keras Model: False
CPU times: user 994 ms, sys: 23.1 ms, total: 1.02 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see eager mode is behind static mode, and by default our model was indeed executing statically, more or less matching explicit static graph execution.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully this has been an illustrative tour of both DRL and the things to come in TensorFlow 2.0. Note that this is still just a nightly preview build, not even a release candidate. Everything is subject to change and if there’s something about TensorFlow you especially dislike (or like :) ) , &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/developers&#34; target=&#34;_blank&#34;&gt;let the developers know&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;A lingering question people might have is if TensorFlow is better than PyTorch? Maybe. Maybe not. Both are great libraries, so it is hard to say one way or the other. If you’re familiar with PyTorch, you probably noticed that TensorFlow 2.0 not only caught up, but also avoided some of the PyTorch API pitfalls.&lt;/p&gt;

&lt;p&gt;In either case what is clear is that this competition has resulted in a net-positive outcome for both camps and I am excited to see what will become of the frameworks in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reaver: Modular Deep Reinforcement Learning</title>
      <link>http://inoryy.com/project/reaver-drl/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0200</pubDate>
      
      <guid>http://inoryy.com/project/reaver-drl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Starter Agent for Coders Strike Back AI Challenge</title>
      <link>http://inoryy.com/project/csb-starter/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0200</pubDate>
      
      <guid>http://inoryy.com/project/csb-starter/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why I Majored in Statistics for a Career in Artificial Intelligence</title>
      <link>http://inoryy.com/post/why-study-statistics-for-artificial-intelligence/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0200</pubDate>
      
      <guid>http://inoryy.com/post/why-study-statistics-for-artificial-intelligence/</guid>
      <description>&lt;p&gt;Undergraduate major is often the first significant career decision a person makes in his life. As artificial intelligence (AI) becomes more and more ingrained in our society, many people begin to consider a career in AI as a viable choice in their life. However, it is still very rare to have an undergraduate degree fully dedicated to AI, so people opt for what they perceive to be the next best thing - computer science. But I believe there is a better alternative: statistics, and in this blog post I will try to explain why, based on my own example.&lt;/p&gt;

&lt;p&gt;In the recent years AI and its many subfields like machine learning (ML) have exploded in popularity and are on track to pretty much take over every industry out there. And people have noticed: the introductory course Machine Learning (CS229) at Stanford had over a thousand students enrolled in the fall of 2017!&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Stanford&amp;#39;s first day of class--record-breaking 1040 people already enrolled for on-campus Machine Learning (CS229). Wow! &lt;a href=&#34;https://twitter.com/danboneh?ref_src=twsrc%5Etfw&#34;&gt;@danboneh&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andrew Ng (@AndrewYNg) &lt;a href=&#34;https://twitter.com/AndrewYNg/status/912382154155352064?ref_src=twsrc%5Etfw&#34;&gt;September 25, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Well, I want to let you in on a little secret: majority of AI/ML, including the hip new trend you’ve probably heard about called deep learning, is just applied statistics in disguise: many ML techniques and algorithms are either fully borrowed from or heavily rely on the theory from statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://xkcd.com/1838/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://imgs.xkcd.com/comics/machine_learning.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, statistics is lacking in their PR department, which causes many people to misunderstand the field. When I said I am majoring in statistics, many of my peers reaction was confusion at best, with some even assuming I was joking. In fact I was often looked down upon by both maths and CS majors: mathematicians considered stats to be not “pure” enough, whereas CS people thought it’s not engineering-oriented enough. What’s funny is that I actually agree with both of those camps, but I believe those to be pros rather than cons. So let’s review some of the core subjects I’ve taken during my undergraduate studies and how they have helped me with AI/ML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mathematical Analysis.&lt;/strong&gt; You’ve probably heard of or even taken the “practical” alternative to it - Calculus, which is an okay subject, but in my opinion by not focusing on the theory behind the various theorems and lemmas a student never actually builds an intuitive understanding. And boy does it help in AI/ML. The topics at the heart of MA - continuity and differentiability, are also what is behind most of AI/ML algorithms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Probability Theory and Statistics.&lt;/strong&gt; Again, some version of this subject is likely taught in CS degrees as well, but theory is typically avoided. I’ve had no trouble diving head first into reinforcement learning thanks to deep and intuitive understanding of random variables and their estimates, expectations, distributions, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Numerical Methods.&lt;/strong&gt; Speaking of what is behind most of AI/ML, this subject tackles the questions of function optimization and approximation. And if function approximation sounds alien to you, then perhaps you’ve heard of its special case - artificial neural networks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Matrix Calculus.&lt;/strong&gt; And while we’re on the subject of artificial neural networks, you’ve probably heard that they are represented as a chain of differentiable matrix operations. Well, here is a whole subject dedicated to understanding how to transfer your multivariate differentiation theory into the world of linear algebra.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo Methods.&lt;/strong&gt; Have you ever wondered how probability theory is applied in practice? How can your computer generate random variables from any distribution? Well, this subject covers this and much more. And if you are into reinforcement learning then this course is probably the most important one to take as it covers a large chunk of the theory behind it. For example, the REINFORCE family of algorithms are built on the monte carlo methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stochastic Processes.&lt;/strong&gt; Speaking of theory behind RL, here&amp;rsquo;s a whole subject dedicated to dealing with probability distributions over time. Markov Chains, Renewals, Queues, Brownian Motions, Gaussian Processes, &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Analysis.&lt;/strong&gt; Did I mention that majority of machine learning is actually applied statistics? This course intimately covers the theory behind what people would refer to as classical ML - from simple linear regression to generalized models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experimental Design.&lt;/strong&gt; As we are starting to reach the limitations of our hardware the various ML/RL experiments become increasingly expensive in terms of wall-clock time. More and more people are now looking for ways to extract similar quality of information with less effort. Well, statisticians have been working on this problem for decades and you can learn all about it in this course.&lt;/p&gt;

&lt;p&gt;But what about programming, you might ask? Well, with a balanced curriculum you actually get quite a fair share of computer science. In fact, with a couple of good elective choices you can cover most of the fundamental knowledge necessary to work as a software engineer if you ever wanted to switch. For example here are the CS courses I have had in my undergrad:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Intro to Programming (Python)&lt;/li&gt;
&lt;li&gt;Object Oriented Programming (Java)&lt;/li&gt;
&lt;li&gt;Algorithms and Data Structures (Java)&lt;/li&gt;
&lt;li&gt;Database Systems (SQL)&lt;/li&gt;
&lt;li&gt;Operating Systems (Python / Java)&lt;/li&gt;
&lt;li&gt;Programming Languages (Prolog, Haskell, Scala, OCaml, C)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So where’s the catch? Well, the problem with having a subject that is so widely misunderstood and unpopular is that the recruiters looking at your resume might assume you’re one of those hippies that prefers pen &amp;amp; paper to a keyboard and pass you over for a &amp;ldquo;safe&amp;rdquo; computer science guy. Unfortunately to get around this I think it’s inevitable that you still have to get the desired stamp, either via double major or with a computer science focused masters degree.&lt;/p&gt;

&lt;p&gt;In conclusion, I believe statistics to be the perfect major for a career in AI. As I am wrapping up my first semester of computer science masters I feel that I am often quite ahead of my peers specifically because of my undergraduate background and hopefully I have persuaded some of you to give it a shot!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
