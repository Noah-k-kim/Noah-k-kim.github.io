<!DOCTYPE html>
<html lang="en-us">
<head>

  
  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-18538959-6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-18538959-6');
    </script>
  
  

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.52" />
  <meta name="author" content="Roman Ring">

  
  
  
  
    
  
  <meta name="description" content="Have you ever wondered how will the machine learning frameworks of the &#39;20s look like?
In this essay, I examine the directions AI research might take and the requirements they impose
on the tools at our disposal, concluding with an overview of what I believe to be the
two strong candidates: `JAX` and `S4TF`.
">

  
  <link rel="alternate" hreflang="en-us" href="http://inoryy.com/post/next-gen-ml-tools/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  <link rel="alternate" href="http://inoryy.com/index.xml" type="application/rss+xml" title="Roman Ring">
  <link rel="feed" href="http://inoryy.com/index.xml" type="application/rss+xml" title="Roman Ring">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="http://inoryy.com/post/next-gen-ml-tools/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@inoryy">
  <meta property="twitter:creator" content="@inoryy">
  
  <meta property="og:site_name" content="Roman Ring">
  <meta property="og:url" content="http://inoryy.com/post/next-gen-ml-tools/">
  <meta property="og:title" content="The Next Generation of Machine Learning Tools | Roman Ring">
  <meta property="og:description" content="Have you ever wondered how will the machine learning frameworks of the &#39;20s look like?
In this essay, I examine the directions AI research might take and the requirements they impose
on the tools at our disposal, concluding with an overview of what I believe to be the
two strong candidates: `JAX` and `S4TF`.
">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-01-22T00:00:00&#43;02:00">
  
  <meta property="article:modified_time" content="2020-01-22T00:00:00&#43;02:00">
  

  

  

  <title>The Next Generation of Machine Learning Tools | Roman Ring</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Roman Ring</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/files/ring_roman_cv.pdf">
            
            <span>CV</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">The Next Generation of Machine Learning Tools</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Roman Ring">
  </span>
  

  <span class="article-date">
    
    <meta content="2020-01-22 00:00:00 &#43;0200 EET" itemprop="datePublished">
    <time datetime="2020-01-22 00:00:00 &#43;0200 EET" itemprop="dateModified">
      Jan 22, 2020
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Roman Ring">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    15 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="http://inoryy.com/post/next-gen-ml-tools/#disqus_thread"></a>
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=The%20Next%20Generation%20of%20Machine%20Learning%20Tools&amp;url=http%3a%2f%2finoryy.com%2fpost%2fnext-gen-ml-tools%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=http%3a%2f%2finoryy.com%2fpost%2fnext-gen-ml-tools%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2finoryy.com%2fpost%2fnext-gen-ml-tools%2f&amp;title=The%20Next%20Generation%20of%20Machine%20Learning%20Tools"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=http%3a%2f%2finoryy.com%2fpost%2fnext-gen-ml-tools%2f&amp;title=The%20Next%20Generation%20of%20Machine%20Learning%20Tools"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=The%20Next%20Generation%20of%20Machine%20Learning%20Tools&amp;body=http%3a%2f%2finoryy.com%2fpost%2fnext-gen-ml-tools%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p><h2>Table of Contents</h2>
<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#tools-in-ml-research">Tools in ML Research</a></li>
<li><a href="#a-need-for-innovation">A Need for Innovation</a></li>
<li><a href="#the-next-generation">The Next Generation</a>
<ul>
<li><a href="#swift-for-tensorflow">Swift for TensorFlow</a></li>
<li><a href="#jax">JAX</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul></li>
</ul>
</nav>

<script type="text/javascript" src="/js/fix-toc.js"></script></p>

<h2 id="introduction">Introduction</h2>

<p>Let&rsquo;s travel back to a simpler time when all everyone talked about in machine learning were SVMs and boosted trees,
while Andrew Ng introduced neural networks as a neat party hat trick you would probably never use in practice<sup class="footnote-ref" id="fnref:mlcoursera"><a href="#fn:mlcoursera">1</a></sup>.</p>

<p>The year is 2012, and computer-vision based competition ImageNet is set to be once again won by the newest ensemble of kernel methods.
That is, of course, until a couple of researchers unveiled AlexNet<sup class="footnote-ref" id="fnref:alexnet"><a href="#fn:alexnet">2</a></sup>, having almost two times lower error rate than the competition,
by using what we now commonly refer to as &ldquo;deep learning.&rdquo;</p>

<p>Many people point to AlexNet as one of the most important scientific breakthroughs of the decade, certainly one that helped change the landscape of ML research.
However, it does not take much to realize that under the hood, it is &ldquo;just&rdquo; a combination of prior iterative improvements, many dating back to the early nineties.
At its core, AlexNet is &ldquo;just&rdquo; a modified LeNet<sup class="footnote-ref" id="fnref:lenet"><a href="#fn:lenet">3</a></sup> with more layers, better weight initialization, activation function, and data augmentation.</p>

<h2 id="tools-in-ml-research">Tools in ML Research</h2>

<p>So what made AlexNet stand out so much? I believe the answer lies in the tools researchers had at their disposal, enabling them to run artificial neural networks on GPU accelerators,
a relatively novel idea at the time.
In fact, Alex Krizhevsky&rsquo;s former colleagues recall that many meetings before the competition consisted of Alex describing his progress with the CUDA quirks and features.</p>

<p>Now let us travel back to 2015 when ML research article submissions started blowing up across the board,
including (re-)emergence of many now promising approaches such as generative adversarial learning, deep reinforcement learning,
meta-learning, self-supervised learning, federated learning, neural architecture search, neural differential equations, neural graph networks, and many more.</p>

<p><img src="https://miro.medium.com/max/770/1*Y-CZdwBP2L_XW1YRLdxt0A.png" alt="" />
<span class="source">Image via <a href="https://medium.com/@dcharrezt/neurips-2019-stats-c91346d31c8f" target="_blank">Charrez, D. (2019)</a>.</span></p>

<p>One could claim that this is just a natural outcome of the AI hype. However, I believe a significant factor was the emergence of the second generation of general-purpose
ML frameworks such as TensorFlow<sup class="footnote-ref" id="fnref:tf"><a href="#fn:tf">4</a></sup> and PyTorch<sup class="footnote-ref" id="fnref:pt"><a href="#fn:pt">5</a></sup>, along with NVIDIA going all-in on AI. The frameworks that existed before, such as Caffe<sup class="footnote-ref" id="fnref:caffe"><a href="#fn:caffe">6</a></sup> and Theano<sup class="footnote-ref" id="fnref:theano"><a href="#fn:theano">7</a></sup>,
were challenging to work with, and awkward to extend, which slowed down the research and development of novel ideas.</p>

<h2 id="a-need-for-innovation">A Need for Innovation</h2>

<p>TensorFlow and PyTorch were undoubtedly a net positive, and the teams worked hard to improve the libraries.
Recently, they delivered TensorFlow 2.0 with a more straightforward interface along with eager mode<sup class="footnote-ref" id="fnref:tfe"><a href="#fn:tfe">8</a></sup>,
and PyTorch 1.0 with JIT compilation of the computation graph<sup class="footnote-ref" id="fnref:ts"><a href="#fn:ts">9</a></sup> as well as support for XLA<sup class="footnote-ref" id="fnref:xla"><a href="#fn:xla">10</a></sup> based accelerators such as TPUs<sup class="footnote-ref" id="fnref:tpu"><a href="#fn:tpu">11</a></sup>.
However, these frameworks are also beginning to reach their limits, forcing researchers into some paths while closing doors on others, just like their predecessors.</p>

<p>High-profile DRL projects such as AlphaStar<sup class="footnote-ref" id="fnref:alphastar"><a href="#fn:alphastar">12</a></sup> and OpenAI Five<sup class="footnote-ref" id="fnref:dota"><a href="#fn:dota">13</a></sup> not only utilized large-scale computational clusters
but also pushed the limits of deep learning architecture components by combining deep transformers, nested recurrent networks, deep residual towers, among others.</p>

<p>In his <a href="https://www.thetimes.co.uk/article/demis-hassabis-interview-the-brains-behind-deepmind-on-the-future-of-artificial-intelligence-mzk0zhsp8" target="_blank">interview with The Times newspaper</a>,
Demis Hassabis has stated that DeepMind will be focusing on applying AI directly for scientific breakthroughs.
We can already see a glimpse of that shift in direction with some of their recent Nature articles on neuroscience<sup class="footnote-ref" id="fnref:dopamine"><a href="#fn:dopamine">14</a></sup> and protein folding<sup class="footnote-ref" id="fnref:alphafold"><a href="#fn:alphafold">15</a></sup>.
Even a brief skim through the publications is enough to see that the projects required some unconventional approaches when it comes to engineering.</p>

<p>At NeurIPS 2019, probabilistic programming and bayesian inference were hot topics, especially uncertainty estimation and causal inference.
Leading AI researchers presented their visions on what the future of ML might look like.
Notably, Yoshua Bengio described transitioning to <a href="https://slideslive.com/38921750/from-system-1-deep-learning-to-system-2-deep-learning" target="_blank">system 2 deep learning</a>
with out-of-distribution generalization, sparse graph networks, and causal reasoning.</p>

<p>To summarize, some of the requirements for next-gen ML tools are:</p>

<ul>
<li>fine-grained control flow use</li>
<li>non-standard optimization loops</li>
<li>higher-order differentiation as a first-class citizen</li>
<li>probabilistic programming as a first-class citizen</li>
<li>support for multiple heterogeneous accelerators in one model</li>
<li>seamless scalability from a single machine to gigantic clusters</li>
</ul>

<p>Ideally, the tools should also maintain a clean, straightforward, and extensible API, enabling scientists to research and develop their ideas rapidly.</p>

<h2 id="the-next-generation">The Next Generation</h2>

<p>The good news is that many candidates already exist today, emerging in response to the needs in scientific computing.
From experimental projects like Zygote.jl<sup class="footnote-ref" id="fnref:zygote"><a href="#fn:zygote">16</a></sup> to even specialized languages, e.g. Halide<sup class="footnote-ref" id="fnref:halide"><a href="#fn:halide">17</a></sup> and DiffTaichi<sup class="footnote-ref" id="fnref:taichi"><a href="#fn:taichi">18</a></sup>.
Interestingly, many projects draw inspiration from the fundamental works done by researchers in the auto-diff community<sup class="footnote-ref" id="fnref:ad-survey"><a href="#fn:ad-survey">19</a></sup>, which evolved in parallel to ML.</p>

<p>Many of them were featured at the recent NeurIPS 2019 <a href="https://program-transformations.github.io/" target="_blank">workshop on program transformations</a>.
The two I am most excited about are S4TF<sup class="footnote-ref" id="fnref:s4tf"><a href="#fn:s4tf">20</a></sup> and JAX<sup class="footnote-ref" id="fnref:jax"><a href="#fn:jax">21</a></sup>.
They both tackle the task of making differentiable programming into an integral part of the toolchain, but in their own ways, almost orthogonal to each other.</p>

<h3 id="swift-for-tensorflow">Swift for TensorFlow</h3>

<p>As the name suggests, S4TF tightly integrates the TensorFlow ML framework with the Swift programming language.
A vote of confidence for the project is that it is led by Chris Lattner, who has authored LLVM<sup class="footnote-ref" id="fnref:llvm"><a href="#fn:llvm">22</a></sup>, Clang<sup class="footnote-ref" id="fnref:clang"><a href="#fn:clang">23</a></sup>, and Swift itself.</p>

<p>Swift is a compiled programming language, and one of its primary selling points is
the powerful type system that is static and inferred. What the last part means in
simpler terms is that Swift encompasses ease of use in languages like Python
with code validations and transformations at compile-time, e.g., as in C++.</p>

<pre><code class="language-swift">let a: Int = 1
let b = 2
let c = &quot;3&quot;

print(a + b)         // 3
print(b + c)         // compilation (!) error
print(String(b) + c) // 23
</code></pre>

<p>Swift features enable the S4TF team to meet quite a few requirements in the next-generation list
by having analysis, verification, and optimization of the computation graph executed with efficient algorithms during compilation.</p>

<p>Crucially, the handling of automatic differentiation is off-loaded to the compiler.</p>

<pre><code class="language-swift">struct Linear: Differentiable {
  var w: Float
  var b: Float

  func callAsFunction(_ x: Float) -&gt; Float {
    return w * x + b
  }
}

let f = Linear(w: 1, b: 2)
let 𝛁f = gradient(at: f) { f in f(3.0) }
print(𝛁f) // TangentVector(w: 3.0, b: 1.0)

let 𝛁f2 = gradient(at: f) { f in f([3.0]) } // compilation (!) error
// error: cannot convert value of type '[Float]' to expected argument type 'Float'
</code></pre>

<p>Of course, TensorFlow itself is very well supported in this case.</p>

<pre><code class="language-swift">import TensorFlow

struct Model: Layer {
    var conv = Conv2D&lt;Float&gt;(filterShape: (5, 5, 6, 16), activation: relu)
    var pool = MaxPool2D&lt;Float&gt;(poolSize: (2, 2), strides: (2, 2))
    var flatten = Flatten&lt;Float&gt;()
    var dense = Dense&lt;Float&gt;(inputSize: 16 * 5 * 5, outputSize: 100, activation: relu)
    var logits = Dense&lt;Float&gt;(inputSize: 100, outputSize: 10, activation: identity)

    @differentiable
    func callAsFunction(_ input: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
        return input.sequenced(through: conv, pool, flatten, dense, logits)
    }
}

var model = Model()
let optimizer = RMSProp(for: model, learningRate: 3e-4, decay: 1e-6)

for batch in CIFAR10().trainDataset.batched(128) {
  let (loss, gradients) = valueWithGradient(at: model) { model in
    softmaxCrossEntropy(logits: model(batch.data), labels: batch.label)
  }
  print(loss)
  optimizer.update(&amp;model, along: gradients)
}
</code></pre>

<p>On the other hand, if a critical feature is proving to be difficult to implement,
having intimate knowledge of the whole pipeline is particularly valuable.
For example, the <a href="https://mlir.llvm.org/" target="_blank">MLIR</a> compiler framework is a direct result of the S4TF efforts.</p>

<p>While differentiable programming is the core goal, S4TF is much more than that with a plan to support the
infrastructure for various next-gen ML tools such as debuggers.
For example, imagine an IDE warning a user that the custom model computation always results in a zero gradient
without even executing it.</p>

<p>Python has an incredible community built around scientific computing and the S4TF team has explicitly
taken the time to embrace it via interoperability.</p>

<pre><code class="language-swift">import Python // All that is necessary to enable the interop.

let np = Python.import(&quot;numpy&quot;) // Can import any Python module.
let plt = Python.import(&quot;matplotlib.pyplot&quot;) 

let x = np.arange(0, 10, 0.01)
plt.plot(x, np.sin(x)) // Can use the modules as if inside Python.
plt.show() // Will show the sin plot, just as you would expect.
</code></pre>

<p>This project is a significant undertaking and still has some ways to go before being ready for production.
However, this is a great time to give it a try for both engineers and researchers and potentially contribute to its development.<br />
Work on S4TF has already produced interesting scientific advancements at the intersection of programming language and auto-diff theory<sup class="footnote-ref" id="fnref:diff-curry"><a href="#fn:diff-curry">24</a></sup>.</p>

<p>One thing that especially stands out for me about S4TF is <a href="https://github.com/tensorflow/community/blob/master/sigs/swift/CHARTER.md" target="_blank">their approach to community outreach</a>.
For example, the core developers hold weekly design sessions, which are open for anyone interested to join and even participate.</p>

<p>To learn more about Swift for TensorFlow, here are some useful resources:</p>

<ul>
<li><a href="https://course.fast.ai/part2.html#lesson-13-basics-of-swift-for-deep-learning" target="_blank">Fast.ai&rsquo;s Lessons 13 and 14</a></li>
<li><a href="https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md" target="_blank">Design Doc: Why Swift For TensorFlow?</a></li>
<li><a href="https://colab.research.google.com/github/tensorflow/swift/blob/master/docs/site/tutorials/model_training_walkthrough.ipynb" target="_blank">Model Training Tutorial</a></li>
<li><a href="https://colab.research.google.com/github/tensorflow/swift/blob/master/notebooks/blank_swift.ipynb" target="_blank">Pre-Built Google Colab Notebook</a></li>
<li><a href="https://github.com/tensorflow/swift-models" target="_blank">Swift Models for Popular Architectures in DL and DRL</a></li>
</ul>

<h3 id="jax">JAX</h3>

<p>JAX is a collection of function transformations such as just-in-time compilation and automatic differentiation,
implemented as a thin wrapper over XLA with an API that is essentially a drop-in replacement for NumPy and SciPy.
In fact, one way to get started with JAX is to think of it as an accelerator backed NumPy.</p>

<pre><code class="language-python">import jax.numpy as np

# Will be seamlessly executed on an accelerator such as GPU/TPU.
x, w, b = np.ones((3, 1000, 1000))
y = np.dot(w, x) + b
</code></pre>

<p>Of course, in reality, JAX is much more than that. To many, it might seem that the project appeared out of thin air,
but the truth is that it is an evolution of over five years of research spanning across three projects.
Notably, JAX emerged from <a href="https://github.com/hips/autograd" target="_blank">Autograd</a> &ndash; a research endeavor into AD of native program code
&ndash; generalizing on its core ideas to support arbitrary transformations.</p>

<pre><code class="language-python">def f(x):
  return np.where(x &gt; 0, x, x / (1 + np.exp(-x)))

# Note: same singular style for the API entry points.
jit_f = jax.jit(f) # Will be 10-100x faster, depending on the accelerator.
grad_f = jax.grad(f) # Will work as expected, handling both branches. 
</code></pre>

<p>Aside from the <code>grad</code> and <code>jit</code> discussed above, there are two more excellent examples of JAX transformations, helping
users to batch-process their data via auto-vectorization of batch dimension (<code>vmap</code>) or across multiple devices (<code>pmap</code>).</p>

<pre><code class="language-python">a = np.ones((100, 300))

def g(vec):
  return np.dot(a, vec)

# Suppose `z` is a batch of 10 samples of 1 x 300 vectors.
z = np.ones((10, 300))

g(z) # Will not work due to (batch) dimension mismatch (100x300 x 10x300).

vec_g = jax.vmap(g)
vec_g(z) # Will work, efficiently propagating through batch dimension.

# Manual solution requires &quot;playing&quot; with matrix transpositions.
np.dot(a, z.T)
</code></pre>

<p>These features might seem confusing at first, but after some practice, they turn into an irreplaceable part of a researcher&rsquo;s toolbox.
They have even inspired recent development of similar functionality in both <a href="https://www.tensorflow.org/api_docs/python/tf/vectorized_map" target="_blank">TensorFlow</a>
and <a href="https://twitter.com/apaszke/status/1219886260296261632" target="_blank">PyTorch</a>.</p>

<p>For the time being, JAX authors seem to be sticking to their core competency when it comes to developing new features.
Of course, a reasonable approach but is also the cause for one of its main drawbacks:
lack of built-in neural network components, aside from the proof-of-concept <a href="https://github.com/google/jax/blob/master/jax/experimental/stax.py" target="_blank">Stax</a>.</p>

<p>Adding higher-level features is something where end-users can potentially step in and contribute, and given JAX&rsquo;s solid foundation,
the task might be easier than it seems. For example, there are now two &ldquo;competing&rdquo; libraries built on top of JAX,
both developed by Google researchers, with differing approaches:
<a href="https://github.com/google/trax" target="_blank">Trax</a> and <a href="https://github.com/google-research/flax/tree/prerelease" target="_blank">Flax</a>.</p>

<pre><code class="language-python"># Trax approach is functional.
# Note: params are stored outside and `forward` is &quot;pure&quot;.

import jax.numpy as np
from trax.layers import base

class Linear(base.Layer):
  def __init__(self, num_units, init_fn):
    super().__init__()
    self.num_units = num_units
    self.init_fn = init_fn

  def forward(self, x, w):
    return np.dot(x, w)

  def new_weights(self, input_signature):
    w = self.init_fn((input_signature.shape, self._num_units))
    return w
</code></pre>

<pre><code class="language-python"># Flax approach is object-oriented, closer to PyTorch style.

import jax.numpy as np
from flax import nn

class Linear(nn.Module):
  def apply(self, x, num_units, init_fn):
    W = self.param('W', (x.shape[-1], num_units), init_fn)
    return np.dot(x, W)
</code></pre>

<p>Even though some might prefer a singular way, endorsed by core developers,
having a diversity of methods is a good indicator that the technology is sound.</p>

<p>There are also some directions in research where JAX features especially shine.
For example, in meta-learning, one common approach to training a meta-learner is by computing the gradients of the inputs.
An alternative method for computing gradients &ndash; forward-mode auto-differentiation &ndash; is necessary to solve this task efficiently,
which is supported out-of-the-box in JAX but is either non-existent or an experimental feature in other libraries.</p>

<p>JAX is perhaps more polished and production-ready than its S4TF counter-part and some of the recent developments coming out of Google Research
rely on it, such as Reformer &ndash; a memory-efficient Transformer model capable of handling context windows of a million words while fitting on a consumer GPU<sup class="footnote-ref" id="fnref:reformer"><a href="#fn:reformer">25</a></sup>,
and Neural Tangents &ndash; a library for complex neural networks of infinite width<sup class="footnote-ref" id="fnref:neural-tangents"><a href="#fn:neural-tangents">26</a></sup>.</p>

<p>The library is further embraced by the broader scientific computing community, used for works in molecular dynamics<sup class="footnote-ref" id="fnref:jax-md"><a href="#fn:jax-md">27</a></sup>
and probabilistic programming<sup class="footnote-ref" id="fnref:numpyro"><a href="#fn:numpyro">28</a></sup>, among others.</p>

<p>To get started with JAX and for further reading, please review the following:</p>

<ul>
<li><a href="https://slideslive.com/38922046/program-transformations-for-ml-3" target="_blank">Talk: Overview by Skye Wanderman-Milne, a core developer</a> (starts at 44:26)</li>
<li><a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html" target="_blank">Notebook: Quickstart, going over fundamental features</a></li>
<li><a href="https://github.com/google/jax/tree/master/cloud_tpu_colabs" target="_blank">Notebook: Cloud TPU Playground</a></li>
<li><a href="https://colinraffel.com/blog/you-don-t-know-jax.html" target="_blank">Blog: You don&rsquo;t know JAX</a></li>
<li><a href="https://rlouf.github.io/post/jax-random-walk-metropolis/" target="_blank">Blog: Massively parallel MCMC with JAX</a></li>
<li><a href="https://blog.evjang.com/2019/11/jaxpt.html" target="_blank">Blog: Differentiable Path Tracing on the GPU/TPU</a></li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>ML research is starting to hit the limits of the tools we currently have at our disposal,
but some new and exciting candidates are right around the corner, such as JAX and S4TF.
If you feel yourself to be more of an engineer than a researcher and wonder whether there is even a place for you at the ML table,
hopefully, the answer is clear: right now is the perfect time to get into it.
Moreover, you have an opportunity to participate on the ground floor of the next generation of ML tools!</p>

<p>Note that this does not mean TensorFlow or PyTorch are going anywhere, not in the near future. There is still much value in these mature,
battle-tested libraries. After all, both JAX and S4TF have parts of TensorFlow under their hoods.
But if you are about to start a new research project or if you feel that you are working around library
limitations more than on your ideas, then maybe give them a try!</p>

<h2 id="references">References</h2>
<div class="footnotes">

<hr />

<ol>
<li id="fn:mlcoursera">Ng, A. (2011). Week 4: Neural Networks. COURSERA: Machine Learning.
 <a class="footnote-return" href="#fnref:mlcoursera">↑</a></li>
<li id="fn:alexnet">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In advances in Neural Information Processing Systems.
 <a class="footnote-return" href="#fnref:alexnet">↑</a></li>
<li id="fn:lenet">LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE.
 <a class="footnote-return" href="#fnref:lenet">↑</a></li>
<li id="fn:tf">Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., &hellip; &amp; Kudlur, M. (2016). Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation.
 <a class="footnote-return" href="#fnref:tf">↑</a></li>
<li id="fn:pt">Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., &hellip; &amp; Desmaison, A. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems.
 <a class="footnote-return" href="#fnref:pt">↑</a></li>
<li id="fn:caffe">Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., &hellip; &amp; Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia.
 <a class="footnote-return" href="#fnref:caffe">↑</a></li>
<li id="fn:theano">Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., &hellip; &amp; Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for scientific computing conference.
 <a class="footnote-return" href="#fnref:theano">↑</a></li>
<li id="fn:tfe">Agrawal, A., Modi, A. N., Passos, A., Lavoie, A., Agarwal, A., Shankar, A., &hellip; &amp; Cai, S. (2019). Tensorflow eager: A multi-stage, python-embedded dsl for machine learning. arXiv preprint arXiv:1903.01855.
 <a class="footnote-return" href="#fnref:tfe">↑</a></li>
<li id="fn:ts">Contributors, PyTorch. (2018). Torch script. URL <a href="https://pytorch.org/docs/stable/jit.html" target="_blank">https://pytorch.org/docs/stable/jit.html</a>
 <a class="footnote-return" href="#fnref:ts">↑</a></li>
<li id="fn:xla">Leary, C., &amp; Wang, T. (2017). XLA: TensorFlow, compiled. TensorFlow Dev Summit.
 <a class="footnote-return" href="#fnref:xla">↑</a></li>
<li id="fn:tpu">Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., &hellip; &amp; Boyle, R. (2017). In-datacenter performance analysis of a tensor processing unit. In 44th Annual International Symposium on Computer Architecture.
 <a class="footnote-return" href="#fnref:tpu">↑</a></li>
<li id="fn:alphastar">Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., &hellip; &amp; Silver, D. (2019). Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature. doi:10.1038/s41586-019-1724-z
 <a class="footnote-return" href="#fnref:alphastar">↑</a></li>
<li id="fn:dota">Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., &hellip; &amp; Józefowicz, R. (2019). Dota 2 with Large Scale Deep Reinforcement Learning. arXiv preprint arXiv:1912.06680.
 <a class="footnote-return" href="#fnref:dota">↑</a></li>
<li id="fn:dopamine">Dabney, W., Kurth-Nelson, Z., Uchida, N., Starkweather, C. K., Hassabis, D., Munos, R., &amp; Botvinick, M. (2020). A distributional code for value in dopamine-based reinforcement learning. Nature. doi: 10.1038/s41586-019-1924-6
 <a class="footnote-return" href="#fnref:dopamine">↑</a></li>
<li id="fn:alphafold">Senior, A., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T., &hellip; &amp; Penedones, H. (2020). Improved protein structure prediction using potentials from deep learning. Nature.
 <a class="footnote-return" href="#fnref:alphafold">↑</a></li>
<li id="fn:zygote">Innes, M. (2018). Don&rsquo;t Unroll Adjoint: Differentiating SSA-Form Programs. arXiv preprint arXiv:1810.07951.
 <a class="footnote-return" href="#fnref:zygote">↑</a></li>
<li id="fn:halide">Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., &amp; Amarasinghe, S. (2013). Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. In ACM Sigplan Notices.
 <a class="footnote-return" href="#fnref:halide">↑</a></li>
<li id="fn:taichi">Hu, Y., Anderson, L., Li, T. M., Sun, Q., Carr, N., Ragan-Kelley, J., &amp; Durand, F. (2019). DiffTaichi: Differentiable Programming for Physical Simulation. arXiv preprint arXiv:1910.00935.
 <a class="footnote-return" href="#fnref:taichi">↑</a></li>
<li id="fn:ad-survey">Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research.
 <a class="footnote-return" href="#fnref:ad-survey">↑</a></li>
<li id="fn:s4tf">Wei, R., &amp; Zheng, D. (2018). Swift for TensorFlow. URL <a href="https://github.com/tensorflow/swift" target="_blank">https://github.com/tensorflow/swift</a>.
 <a class="footnote-return" href="#fnref:s4tf">↑</a></li>
<li id="fn:jax">Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., &amp; Wanderman-Milne, S (2018). JAX: composable transformations of Python+ NumPy programs. URL <a href="https://github.com/google/jax" target="_blank">https://github.com/google/jax</a>.
 <a class="footnote-return" href="#fnref:jax">↑</a></li>
<li id="fn:llvm">Lattner, C. (2002). LLVM: An infrastructure for multi-stage optimization. Masters thesis, University of Illinois.
 <a class="footnote-return" href="#fnref:llvm">↑</a></li>
<li id="fn:clang">Lattner, C. (2008). LLVM and Clang: Next generation compiler technology. In The BSD conference.
 <a class="footnote-return" href="#fnref:clang">↑</a></li>
<li id="fn:diff-curry">Vytiniotis, D., Belov, D., Wei, R., Plotkin, G., &amp; Abadi, M. (2019). The Differentiable Curry.
 <a class="footnote-return" href="#fnref:diff-curry">↑</a></li>
<li id="fn:reformer">Kitaev, N., Kaiser, L., and Levskaya, A. (2020). Reformer: The Efficient Transformer. In International Conference on Learning Representations.
 <a class="footnote-return" href="#fnref:reformer">↑</a></li>
<li id="fn:neural-tangents">Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A., Sohl-dickstein, J., &amp; Schoenholz, S. (2020). Neural Tangents: Fast and Easy Infinite Neural Networks in Python. In International Conference on Learning Representations.
 <a class="footnote-return" href="#fnref:neural-tangents">↑</a></li>
<li id="fn:jax-md">Schoenholz, S., &amp; Cubuk, E. (2020). JAX, MD End-to-End Differentiable, Hardware Accelerated, Molecular Dynamics in Pure Python. Bulletin of the American Physical Society.
 <a class="footnote-return" href="#fnref:jax-md">↑</a></li>
<li id="fn:numpyro">Phan, D., Pradhan, N., &amp; Jankowiak, M. (2019). Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro.
 <a class="footnote-return" href="#fnref:numpyro">↑</a></li>
</ol>
</div>

    </div>

    





    
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "inoryy" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; 2018&ndash;2020 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/cpp.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//inoryy.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

